{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis — Water Quality Prediction\n",
    "\n",
    "This notebook explores the training data, feature distributions, spatial patterns, \n",
    "and relationships between predictors and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"datasets\"\n",
    "print(\"Datasets available:\", [f.name for f in DATA_DIR.iterdir() if f.is_file()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR / \"water_quality_training_dataset.csv\")\n",
    "train[\"Sample Date\"] = pd.to_datetime(train[\"Sample Date\"], dayfirst=True)\n",
    "\n",
    "val = pd.read_csv(DATA_DIR / \"submission_template.csv\")\n",
    "val[\"Sample Date\"] = pd.to_datetime(val[\"Sample Date\"], dayfirst=True)\n",
    "\n",
    "print(f\"Training:   {train.shape[0]} rows x {train.shape[1]} cols\")\n",
    "print(f\"Validation: {val.shape[0]} rows x {val.shape[1]} cols\")\n",
    "print(f\"\\nTraining date range: {train['Sample Date'].min()} to {train['Sample Date'].max()}\")\n",
    "print(f\"Validation date range: {val['Sample Date'].min()} to {val['Sample Date'].max()}\")\n",
    "print(f\"\\nTraining columns: {list(train.columns)}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\"]\n",
    "\n",
    "print(\"Target statistics:\")\n",
    "print(train[targets].describe().round(2))\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "print(train[targets].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for i, col in enumerate(targets):\n",
    "    axes[i].hist(train[col].dropna(), bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel(\"Value\")\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "    axes[i].axvline(train[col].median(), color=\"red\", linestyle=\"--\", label=f\"Median: {train[col].median():.1f}\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Target Variable Distributions\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/figures/target_distributions.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transformed DRP — check if log helps the bimodal distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "drp = train[\"Dissolved Reactive Phosphorus\"].dropna()\n",
    "axes[0].hist(drp, bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].set_title(\"DRP — Original\")\n",
    "\n",
    "axes[1].hist(np.log1p(drp), bins=50, edgecolor=\"black\", alpha=0.7, color=\"orange\")\n",
    "axes[1].set_title(\"DRP — Log1p Transformed\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train[targets].corr()\n",
    "print(\"Target correlations:\")\n",
    "print(corr.round(3))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(corr, annot=True, cmap=\"RdBu_r\", center=0, vmin=-1, vmax=1, ax=ax)\n",
    "ax.set_title(\"Target Variable Correlations\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique stations\n",
    "train_stations = train.groupby([\"Latitude\", \"Longitude\"]).agg(\n",
    "    n_samples=(\"Total Alkalinity\", \"count\"),\n",
    "    mean_ta=(\"Total Alkalinity\", \"mean\"),\n",
    "    mean_ec=(\"Electrical Conductance\", \"mean\"),\n",
    "    mean_drp=(\"Dissolved Reactive Phosphorus\", \"mean\"),\n",
    ").reset_index()\n",
    "\n",
    "val_stations = val[[\"Latitude\", \"Longitude\"]].drop_duplicates()\n",
    "\n",
    "print(f\"Training stations: {len(train_stations)}\")\n",
    "print(f\"Validation stations: {len(val_stations)}\")\n",
    "print(f\"\\nTraining lat range: {train_stations.Latitude.min():.2f} to {train_stations.Latitude.max():.2f}\")\n",
    "print(f\"Training lon range: {train_stations.Longitude.min():.2f} to {train_stations.Longitude.max():.2f}\")\n",
    "print(f\"Validation lat range: {val_stations.Latitude.min():.2f} to {val_stations.Latitude.max():.2f}\")\n",
    "print(f\"Validation lon range: {val_stations.Longitude.min():.2f} to {val_stations.Longitude.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map of training vs validation stations\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    train_stations[\"Longitude\"], train_stations[\"Latitude\"],\n",
    "    c=train_stations[\"mean_ta\"], cmap=\"viridis\",\n",
    "    s=train_stations[\"n_samples\"] * 2, alpha=0.7,\n",
    "    edgecolors=\"black\", linewidth=0.5, label=\"Training\"\n",
    ")\n",
    "ax.scatter(\n",
    "    val_stations[\"Longitude\"], val_stations[\"Latitude\"],\n",
    "    c=\"red\", s=80, marker=\"^\", edgecolors=\"black\",\n",
    "    linewidth=0.5, label=\"Validation\", zorder=5\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, label=\"Mean Total Alkalinity\", shrink=0.7)\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_title(\"Station Locations — Training (circles) vs Validation (triangles)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/figures/station_map.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples per station distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "train_stations[\"n_samples\"].hist(bins=30, edgecolor=\"black\", ax=ax)\n",
    "ax.set_xlabel(\"Number of Samples per Station\")\n",
    "ax.set_ylabel(\"Number of Stations\")\n",
    "ax.set_title(\"Samples per Station\")\n",
    "ax.axvline(train_stations[\"n_samples\"].median(), color=\"red\", linestyle=\"--\",\n",
    "           label=f\"Median: {train_stations['n_samples'].median():.0f}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"month\"] = train[\"Sample Date\"].dt.month\n",
    "train[\"year\"] = train[\"Sample Date\"].dt.year\n",
    "\n",
    "# Monthly mean for each target\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "month_names = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n",
    "               \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "\n",
    "for i, col in enumerate(targets):\n",
    "    monthly = train.groupby(\"month\")[col].agg([\"mean\", \"std\"]).reset_index()\n",
    "    axes[i].bar(monthly[\"month\"], monthly[\"mean\"], yerr=monthly[\"std\"],\n",
    "               capsize=3, alpha=0.7, edgecolor=\"black\")\n",
    "    axes[i].set_xticks(range(1, 13))\n",
    "    axes[i].set_xticklabels(month_names, rotation=45)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_ylabel(\"Mean Value\")\n",
    "\n",
    "plt.suptitle(\"Monthly Average by Target (South Africa: DJF=Summer/Wet, JJA=Winter/Dry)\", fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/figures/monthly_patterns.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly trends\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for i, col in enumerate(targets):\n",
    "    yearly = train.groupby(\"year\")[col].agg([\"mean\", \"std\"]).reset_index()\n",
    "    axes[i].bar(yearly[\"year\"], yearly[\"mean\"], yerr=yearly[\"std\"],\n",
    "               capsize=3, alpha=0.7, edgecolor=\"black\")\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_ylabel(\"Mean Value\")\n",
    "    axes[i].set_xlabel(\"Year\")\n",
    "\n",
    "plt.suptitle(\"Yearly Average by Target\", fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Analysis — Landsat Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat = pd.read_csv(DATA_DIR / \"train_landsat_features.csv\")\n",
    "landsat[\"Sample Date\"] = pd.to_datetime(landsat[\"Sample Date\"], dayfirst=True)\n",
    "\n",
    "print(f\"Landsat shape: {landsat.shape}\")\n",
    "print(f\"Columns: {list(landsat.columns)}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(landsat.isna().sum())\n",
    "print(f\"\\nMissing %:\")\n",
    "print((landsat.isna().sum() / len(landsat) * 100).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge landsat with targets for correlation analysis\n",
    "merged = train.merge(landsat, on=[\"Latitude\", \"Longitude\", \"Sample Date\"], how=\"left\")\n",
    "\n",
    "band_cols = [\"nir\", \"green\", \"swir16\", \"swir22\", \"NDMI\", \"MNDWI\"]\n",
    "corr_with_targets = merged[band_cols + targets].corr().loc[band_cols, targets]\n",
    "\n",
    "print(\"Feature-Target correlations:\")\n",
    "print(corr_with_targets.round(3))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.heatmap(corr_with_targets, annot=True, cmap=\"RdBu_r\", center=0, vmin=-1, vmax=1, ax=ax)\n",
    "ax.set_title(\"Landsat Band Correlations with Targets\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Analysis — TerraClimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = pd.read_csv(DATA_DIR / \"train_terraclimate_features.csv\")\n",
    "tc[\"Sample Date\"] = pd.to_datetime(tc[\"Sample Date\"], dayfirst=True)\n",
    "\n",
    "print(f\"TerraClimate shape: {tc.shape}\")\n",
    "print(f\"Columns: {list(tc.columns)}\")\n",
    "print(f\"\\nPET stats:\")\n",
    "print(tc[\"pet\"].describe().round(2))\n",
    "\n",
    "# Merge and check correlation\n",
    "merged_tc = train.merge(tc, on=[\"Latitude\", \"Longitude\", \"Sample Date\"], how=\"left\")\n",
    "for col in targets:\n",
    "    r = merged_tc[[\"pet\", col]].dropna().corr().iloc[0, 1]\n",
    "    print(f\"\\nPET vs {col}: r = {r:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if extended TerraClimate exists\n",
    "tc_ext_path = DATA_DIR / \"processed\" / \"train_terraclimate_extended.csv\"\n",
    "if tc_ext_path.exists():\n",
    "    tc_ext = pd.read_csv(tc_ext_path)\n",
    "    tc_ext[\"Sample Date\"] = pd.to_datetime(tc_ext[\"Sample Date\"])\n",
    "    print(f\"Extended TerraClimate: {tc_ext.shape}\")\n",
    "    print(f\"Variables: {[c for c in tc_ext.columns if c not in ['Latitude', 'Longitude', 'Sample Date']]}\")\n",
    "\n",
    "    # Correlations with all climate variables\n",
    "    climate_cols = [c for c in tc_ext.columns if c not in [\"Latitude\", \"Longitude\", \"Sample Date\"]]\n",
    "    merged_ext = train.merge(tc_ext, on=[\"Latitude\", \"Longitude\", \"Sample Date\"], how=\"left\")\n",
    "    corr_climate = merged_ext[climate_cols + targets].corr().loc[climate_cols, targets]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(corr_climate, annot=True, cmap=\"RdBu_r\", center=0, fmt=\".2f\", ax=ax)\n",
    "    ax.set_title(\"TerraClimate Variable Correlations with Targets\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../outputs/figures/climate_target_correlations.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Extended TerraClimate not yet extracted. Run: python src/climate_extractor.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Spatial Overlap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# How far are validation stations from the nearest training station?\n",
    "train_coords = np.radians(train_stations[[\"Latitude\", \"Longitude\"]].values)\n",
    "val_coords = np.radians(val_stations.values)\n",
    "\n",
    "tree = cKDTree(train_coords)\n",
    "dists, idxs = tree.query(val_coords, k=1)\n",
    "\n",
    "# Convert radians to approximate km (Earth radius ~6371 km)\n",
    "dist_km = dists * 6371\n",
    "\n",
    "print(\"Nearest training station distance for each validation station:\")\n",
    "print(f\"  Min:    {dist_km.min():.1f} km\")\n",
    "print(f\"  Median: {np.median(dist_km):.1f} km\")\n",
    "print(f\"  Mean:   {dist_km.mean():.1f} km\")\n",
    "print(f\"  Max:    {dist_km.max():.1f} km\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.bar(range(len(dist_km)), sorted(dist_km), edgecolor=\"black\", alpha=0.7)\n",
    "ax.set_xlabel(\"Validation Station (sorted)\")\n",
    "ax.set_ylabel(\"Distance to Nearest Training Station (km)\")\n",
    "ax.set_title(\"Spatial Gap: How far are validation sites from training data?\")\n",
    "ax.axhline(np.median(dist_km), color=\"red\", linestyle=\"--\", label=f\"Median: {np.median(dist_km):.0f} km\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/figures/spatial_gap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inter-Station Variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much variability is between-station vs within-station?\n",
    "for col in targets:\n",
    "    total_var = train[col].var()\n",
    "    between_var = train.groupby([\"Latitude\", \"Longitude\"])[col].mean().var()\n",
    "    within_var = train.groupby([\"Latitude\", \"Longitude\"])[col].var().mean()\n",
    "\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Total variance:   {total_var:.2f}\")\n",
    "    print(f\"  Between-station:  {between_var:.2f} ({between_var/total_var*100:.1f}%)\")\n",
    "    print(f\"  Within-station:   {within_var:.2f} ({within_var/total_var*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n=> High between-station variance means location-specific features (terrain, land cover, soil) are critical.\")\n",
    "print(\"=> High within-station variance means temporal features (climate, seasons) matter too.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Availability Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = DATA_DIR / \"processed\"\n",
    "\n",
    "features_status = {\n",
    "    \"Landsat (benchmark)\": (DATA_DIR / \"train_landsat_features.csv\").exists(),\n",
    "    \"TerraClimate PET (benchmark)\": (DATA_DIR / \"train_terraclimate_features.csv\").exists(),\n",
    "    \"TerraClimate Extended (14 vars)\": (processed / \"train_terraclimate_extended.csv\").exists(),\n",
    "    \"Terrain (SRTM elevation)\": (processed / \"terrain_features.csv\").exists(),\n",
    "    \"Land Cover (ESA WorldCover)\": (processed / \"landcover_features.csv\").exists(),\n",
    "    \"Soil (SoilGrids)\": (processed / \"soil_features.csv\").exists(),\n",
    "}\n",
    "\n",
    "print(\"Feature Dataset Status:\")\n",
    "print(\"=\" * 50)\n",
    "for name, available in features_status.items():\n",
    "    status = \"READY\" if available else \"NOT EXTRACTED\"\n",
    "    print(f\"  {name:40s} {status}\")\n",
    "\n",
    "n_ready = sum(features_status.values())\n",
    "print(f\"\\n{n_ready}/{len(features_status)} feature sources available.\")\n",
    "\n",
    "if n_ready < len(features_status):\n",
    "    print(\"\\nTo extract missing features, run:\")\n",
    "    if not features_status[\"TerraClimate Extended (14 vars)\"]:\n",
    "        print(\"  python src/climate_extractor.py --split both\")\n",
    "    if not features_status[\"Terrain (SRTM elevation)\"]:\n",
    "        print(\"  python src/terrain_extractor.py\")\n",
    "    if not features_status[\"Land Cover (ESA WorldCover)\"]:\n",
    "        print(\"  python src/landcover_extractor.py\")\n",
    "    if not features_status[\"Soil (SoilGrids)\"]:\n",
    "        print(\"  python src/soil_extractor.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Baseline Model Check (Spatial CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "from data_loader import build_full_dataset\n",
    "from feature_builder import build_features, get_feature_columns\n",
    "from model_trainer import assign_spatial_groups\n",
    "from evaluation import spatial_cv_report\n",
    "\n",
    "# Load full dataset with all available features\n",
    "train_full = build_full_dataset(\"train\")\n",
    "train_full = build_features(train_full, is_training=True)\n",
    "\n",
    "feature_cols = get_feature_columns(train_full)\n",
    "print(f\"Total features available: {len(feature_cols)}\")\n",
    "for c in feature_cols:\n",
    "    missing_pct = train_full[c].isna().sum() / len(train_full) * 100\n",
    "    print(f\"  {c:30s} {missing_pct:5.1f}% missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = train_full[feature_cols]\n",
    "groups = assign_spatial_groups(train_full)\n",
    "\n",
    "print(\"Spatial CV with Random Forest (baseline check):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for target in targets:\n",
    "    y = train_full[target]\n",
    "\n",
    "    def rf_factory():\n",
    "        return RandomForestRegressor(n_estimators=200, max_depth=15,\n",
    "                                     min_samples_leaf=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "    print(f\"\\n--- {target} ---\")\n",
    "    report, oof = spatial_cv_report(rf_factory, X, y, groups)\n",
    "    print(report.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings to guide feature engineering and modeling:\n",
    "\n",
    "1. **Spatial gap**: Validation stations are in a different region — models must generalize spatially\n",
    "2. **Between-station variance**: How much of the signal comes from location vs. time?\n",
    "3. **DRP distribution**: Likely bimodal — consider log1p transform\n",
    "4. **Missing Landsat data**: Cloud cover creates informative missingness\n",
    "5. **Feature extraction status**: Check which data sources still need extraction\n",
    "6. **Baseline spatial CV**: Compare with benchmark R² ~0.20 on leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
